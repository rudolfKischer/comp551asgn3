{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Classification of Image Data\n",
    "- COMP 551 Winter 2024, Mcgill University\n",
    "- Rudolf Kischer: 260956107\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synopsis\n",
    "- In this miniproject, we will implement a multilayer perceptron from scratch, and use it to classify image data. The goal is to implement a basic neural network and its training algorithm from scratch and get hands-on experience with important decisions that you have to make while training these models. You will also have a chance to experiment with convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "- [Sign Language MNIST](https://www.kaggle.com/datasets/datamunge/sign-language-mnist/data)\n",
    "- Features: 28 x 28 grayscale images of hand symbols (784 pixels/features)\n",
    "- Labels: 24 classes of letters (excludes 9=J and 25=Z because they require motion)\n",
    "- Train: 27,455\n",
    "- Test: 7172 \n",
    "- Most of the images are produced through an alteration of 1704 uncropped color images\n",
    "- These alternations include the following:\n",
    "    - \"To create new data, an image pipeline was used based on ImageMagick and included cropping to hands-only, gray-scaling, resizing, and then creating at least 50+ variations to enlarge the quantity. The modification and expansion strategy was filters ('Mitchell', 'Robidoux', 'Catrom', 'Spline', 'Hermite'), along with 5% random pixelation, +/- 15% brightness/contrast, and finally 3 degrees rotation.\"\n",
    "- CSV format, (label, pixel1, pixel2, ... , pixel784)\n",
    "- <img width=400 src=\"https://storage.googleapis.com/kagglesdsdata/datasets/3258/5337/amer_sign3.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com/20240314/auto/storage/goog4_request&X-Goog-Date=20240314T192335Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=5090d6842cb28ba5080a37a44706cfab4cba880c104d7acf1510df2a187f3c644bccde7d786cf964d8704f172a1b288bff914ae767deace400edfbd0610023d7cc6c6e329c2d365dc9f5a81c6bfe641800d6c7ecb500470fb48cabf2b555080be0f07559522be5487e6f3f456e8c20b909a818ffd6eaf2658089c82659443e1df42d0c06956fd5f46d9d1b9dfd6458ab03e47796b278463a2d1ebbeac2328b7ba668662807ce3b138e72afca7e9f29d4d01854d0ed4e8416afc4206787976e861cc0f14d9755542f06ee1a52e71e16a112f7e2e1e53a6136d711f54a64e8ad531c07083108fd034a1bf8cf04a5c9a13f94ca6fa0291fb1c60dc9b7629095b1a9\"/>\n",
    "- <img width=400 src=\"https://storage.googleapis.com/kagglesdsdata/datasets/3258/5337/american_sign_language.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2@kaggle-161607.iam.gserviceaccount.com/20240315/auto/storage/goog4_request&X-Goog-Date=20240315T200917Z&X-Goog-Expires=345600&X-Goog-SignedHeaders=host&X-Goog-Signature=208afda814e246ab63f6d5f39436a53bcb0aa69be2ca78290e4caf450a4b47f8ea3fd9686815a4836e37c35682e0cc70c122f52f664ee18fa36407caa91f3f00b3874aa926ca4fab2d2366e114da10cad6014c1e8d978a80a150c45e3b4b5a855756a5d8ac9e1c606674728b5868a48e954329c9b41af9a3a0b912fedecf4d2bca40407add7f87d4c4bd57a423dbb4257b73fe0bf5830b81eadea549a41dd70b47c9acc9150078416f517b2814578506b379aee8543fe99f8e060ac978dd21dfc9dab5d7702a1d16b9ccc330500f9204e43ca21462f6bb3f48a70a70c7445f88a0a02e96a587c4babb965eb12adc6b2ca82e3cd6e91026f5204e93edc8bb82f2\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    " \n",
    "\n",
    "\n",
    "def interactive_plot(amplitude, frequency):\n",
    "    x = np.linspace(0, 2 * np.pi, 1000)\n",
    "    y = amplitude * np.sin(frequency * x)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('X-axis')\n",
    "    plt.ylabel('Y-axis')\n",
    "    plt.title('Interactive Sine Wave')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "interact(interactive_plot, amplitude=(1, 5, 0.1), frequency=(1, 10, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "- To prepare the data for usage we need to:\n",
    "  - download the dataset\n",
    "  - load the dataset\n",
    "  - seperate into X and Y\n",
    "  - vectorize the image data\n",
    "  - Center and normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_mnist_sign_language():\n",
    "  kaggle_command = f'kaggle datasets download -d datamunge/sign-language-mnist'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dataset(csv_path):\n",
    "  df = pd.read_csv(csv_path)\n",
    "  return df\n",
    "\n",
    "def load_mnist_sign_dataset():\n",
    "  dataset_directory = 'data/archive/'\n",
    "  test_set_path = f'{dataset_directory}/sign_mnist_test/sign_mnist_test.csv'\n",
    "  train_set_path = f'{dataset_directory}/sign_mnist_train/sign_mnist_train.csv'\n",
    "  df_test = pd.read_csv(test_set_path)\n",
    "  df_train = pd.read_csv(train_set_path)\n",
    "  # df_train.info()\n",
    "  # df_test.info()\n",
    "  return df_test, df_train\n",
    "\n",
    "def shape(df):\n",
    "  Y = df['label']\n",
    "  lb = LabelBinarizer()\n",
    "  Y = lb.fit_transform(Y)\n",
    "  X = df.drop(['label'],axis=1)\n",
    "  X = X.values.reshape(-1,28,28,1)\n",
    "  # one hot encode\n",
    "\n",
    "  # print the shape of Y\n",
    "  print(f'Y shape: {Y.shape}')\n",
    "  \n",
    "  return X, Y\n",
    "\n",
    "def standardize_data(X):\n",
    "  # center by subtracting the mean\n",
    "  # divide by the standard deviation\n",
    "  # N X D \n",
    "  # N X W X H X C\n",
    "  # copy the data\n",
    "  X = X.copy()\n",
    "  X = X - X.mean(axis=0)\n",
    "  X = X / X.std(axis=0)\n",
    "  return X\n",
    "\n",
    "def display_image(X, Y, index):\n",
    "  plt.imshow(X[index].reshape(28,28), cmap='gray')\n",
    "  alphanumeric = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "  plt.title(f'{alphanumeric[np.argmax(Y[index])]}')\n",
    "  plt.show()\n",
    "\n",
    "def display_images(X, Y, indices, title=None):\n",
    "  # do a multiplot\n",
    "  # that displays the the first 9 images\n",
    "  # with their labels\n",
    "  fig, ax = plt.subplots(3, 3, figsize=(10,10))\n",
    "  # add title\n",
    "  if title:\n",
    "    fig.suptitle(title)\n",
    "  alphanumeric = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "  for i in range(3):\n",
    "    for j in range(3):\n",
    "      ax[i,j].imshow(X[indices[i*3+j]].reshape(28,28), cmap='gray')\n",
    "      ax[i,j].set_title(f'{indices[i*3+j]}: {alphanumeric[np.argmax(Y[indices[i*3+j]])]}')\n",
    "\n",
    "  \n",
    "\n",
    "test, train = load_mnist_sign_dataset()\n",
    "X_e, Y_e = shape(test)\n",
    "X_t, Y_t = shape(train)\n",
    "\n",
    "# display_images(X_e, Y_e, [0,1,2,3,4,5,6,7,8], 'Test Set')\n",
    "# display_images(X_t, Y_t, [0,1,2,3,4,5,6,7,8], 'Train Set')\n",
    "\n",
    "\n",
    "# X_e = standardize_data(X_e)\n",
    "X_t = standardize_data(X_t)\n",
    "\n",
    "#display first 5 images\n",
    "# display_images(X_e, Y_e, [0,1,2,3,4,5,6,7,8], 'Test Set Standardized')\n",
    "display_images(X_t, Y_t, [0,1,2,3,4,5,6,7,8], 'Train Set Standardized')\n",
    "\n",
    "# print the dimensions\n",
    "print(f'Test Set: X:{X_e.shape} Y:{Y_e.shape}')\n",
    "print(f'Train Set: X:{X_t.shape} Y:{Y_t.shape}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "- We will be implementing a deep multi layered perceptron\n",
    "- We want it to be custimizable for different depths and breadths as well as activation functions for experimentations\n",
    "- We will also want to implement convolutional layers as well to test the performance increase\n",
    "- Each model will be composed of multiple layers, each which has a forward and backward function to update the weights of that layer\n",
    "- [MLP code Example](https://colab.research.google.com/github/yueliyl/comp551-notebooks/blob/master/MLP.ipynb)\n",
    "- [Deep MLP code Example](https://colab.research.google.com/github/yueliyl/comp551-notebooks/blob/master/NumpyDeepMLP.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here a_j is the the output of the pre-activate hidden unit Zl_k\n",
    "# where l is the lth layer and k is the kth unit in the lth layer\n",
    "# A_l is an K_l x 1 vector where K_l is the number of units in the lth layer\n",
    "# A_l is the pre-activated output of the lth layer\n",
    "# Z_l is the post activated output of the lth layer\n",
    "# Z_l = phi_l(A_l) where phi_l is the activation function of the lth layer\n",
    "\n",
    "def relu(A_l):\n",
    "  return np.maximum(0, A_l)\n",
    "\n",
    "def heaviside(A_l):\n",
    "  return np.heaviside(A_l, 0)\n",
    "\n",
    "def sigmoid(A_l):\n",
    "  return 1 / (1 + np.exp(-A_l))\n",
    "\n",
    "def leaky_relu(A_l, alpha=0.01):\n",
    "  return np.maximum(alpha * A_l, A_l)\n",
    "\n",
    "def swish(A_l, beta=1):\n",
    "  return A_l / (1 + np.exp(-beta * A_l))\n",
    "\n",
    "def softmax(A_l):\n",
    "  expA = np.exp(A_l)\n",
    "  return expA / expA.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A_l is the pre-activated output of the lth layer\n",
    "# it is a K_l x 1 vector, so these derivatives output a gradient\n",
    "\n",
    "# we want to ouput a vector of the same shape as A_l\n",
    "def d_relu(A_l):\n",
    "  return np.heaviside(A_l, 0)\n",
    "\n",
    "def d_heaviside(A_l):\n",
    "  return np.heaviside(A_l, 0)\n",
    "\n",
    "def d_sigmoid(A_l):\n",
    "  return A_l * (1 - A_l)\n",
    "\n",
    "def d_leaky_relu(A_l, alpha=0.01):\n",
    "  return np.maximum(alpha, np.heaviside(A_l, 0))\n",
    "\n",
    "def d_swish(A_l, beta=1):\n",
    "  return swish(A_l, beta) + beta * A_l * (1 - swish(A_l, beta))\n",
    "\n",
    "def d_numerical(A_l, phi, h=1e-5):\n",
    "  return (phi(A_l + h) - phi(A_l - h)) / (2 * h)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cross_entropy(Y, Y_hat):\n",
    "  # -1 dot Y (*) log(Y_hat)\n",
    "  # (*) element wise multiplication\n",
    "  # Y is the output layer activations of the nework\n",
    "  # Y_hat is the predicted output layer activations\n",
    "  CE = -np.ones(Y.shape).dot(Y * np.log(Y_hat))\n",
    "  return CE\n",
    "\n",
    "def mean_squared_error(Y, Y_hat):\n",
    "  return np.mean((Y - Y_hat)**2)\n",
    "\n",
    "def d_cross_entropy(Y, Y_hat):\n",
    "  # -Y / Y_hat\n",
    "  return -Y / Y_hat\n",
    "\n",
    "def d_mean_squared_error(Y, Y_hat):\n",
    "  return Y - Y_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetLayer:\n",
    "    def __init__(self):\n",
    "        self.gradient = None\n",
    "        self.parameters = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class LinearLayer(NeuralNetLayer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.ni = input_size\n",
    "        self.no = output_size\n",
    "        self.w = np.random.randn(output_size, input_size)\n",
    "        self.b = np.random.randn(output_size)\n",
    "        self.cur_input = None\n",
    "        self.parameters = [self.w, self.b]\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.cur_input = x\n",
    "        # n is the number of units in the previous layer\n",
    "        # m is the number of units in the next layer\n",
    "        # x_i = n x 1\n",
    "        # W : (m x n)\n",
    "        # x : (p x n)\n",
    "        # W_ : (1 x m x n)\n",
    "        # x_ : (p x n x 1)\n",
    "        # W_ @ x_ : (1 x m x n) @ (p x n x 1) -> broadcast-> (p x m x n) @ (p x n x 1) = (p x m x 1) -> [(mx1) ...<-p->... (mx1)]\n",
    "        # = (p x m x 1).squeeze() -> (p x m)\n",
    "        # (W_ @ x_) + b = (p x m) + (1 x m) -> brodcast -> (p x m) + (p x m) = (p x m) )\n",
    "        # print(f'w shape: {self.w.shape}')\n",
    "        # print(f'w: {self.w}')\n",
    "        return (self.w[None, :, :] @ x[:, :, None]).squeeze() + self.b\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.cur_input is not None, \"Must call forward before backward\"\n",
    "        #dw = gradient.dot(self.cur_input)\n",
    "        dw = gradient[:, :, None] @ self.cur_input[:, None, :]\n",
    "        db = gradient\n",
    "        self.gradient = [dw, db]\n",
    "        return gradient.dot(self.w)\n",
    "    \n",
    "\n",
    "\n",
    "class ReLULayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.gradient = np.where(x > 0, 1.0, 0.0)\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, gradient):\n",
    "        assert self.gradient is not None, \"Must call forward before backward\"\n",
    "        return gradient * self.gradient\n",
    "    \n",
    "class SoftmaxOutputLayer(NeuralNetLayer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cur_probs = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Subtract the maximum value in each row for numerical stability\n",
    "        x_max = np.max(x, axis=1, keepdims=True)\n",
    "        expX = np.exp(x - x_max)\n",
    "        \n",
    "        # Compute softmax probabilities\n",
    "        self.cur_probs = expX / expX.sum(axis=1, keepdims=True)\n",
    "        return self.cur_probs\n",
    "\n",
    "    def backward(self, target):\n",
    "        assert self.cur_probs is not None, \"Must call forward before backward\"\n",
    "        return self.cur_probs - target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "class MLP:\n",
    "    def __init__(self, *args: List[NeuralNetLayer]):\n",
    "        self.layers = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, target):\n",
    "        for layer in self.layers[::-1]:\n",
    "            target = layer.backward(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, net: MLP):\n",
    "        self.net = net\n",
    "\n",
    "    def step(self):\n",
    "        for layer in self.net.layers[::-1]:\n",
    "            if layer.parameters is not None:\n",
    "                self.update(layer.parameters, layer.gradient)\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "class GradientDescentOptimizer(Optimizer):\n",
    "    def __init__(self, net: MLP, lr: float):\n",
    "        super().__init__(net)\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, gradient):\n",
    "        for (p, g) in zip(params, gradient):\n",
    "            p -= self.lr * g.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ce_loss(y, y_hat):\n",
    "    epsilon = 1e-10\n",
    "    return -(y * np.log(y_hat + epsilon)).sum(axis=-1).mean()\n",
    "\n",
    "def plot_losses(loss_dict):\n",
    "    for name, loss in loss_dict.items():\n",
    "        plt.plot(loss, label=name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "def train(mlp: MLP, optimizer: Optimizer, data_x, data_y, steps):\n",
    "    losses = []\n",
    "    v_losses = []\n",
    "    # labels = np.eye(3)[np.array(data_y)]\n",
    "    labels = data_y\n",
    "\n",
    "    # validation set 0.2% split of train\n",
    "    # make sure its shuffled\n",
    "    shuffled_data_x = data_x.copy()\n",
    "    # shuffle only the rows\n",
    "    np.random.shuffle(shuffled_data_x)\n",
    "    split = int(len(shuffled_data_x) * 0.2)\n",
    "    validation_x = shuffled_data_x[:split]\n",
    "    validation_y = labels[:split]\n",
    "    data_x = shuffled_data_x[split:]\n",
    "    labels = labels[split:]\n",
    "\n",
    "    # Create a figure and axes for plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    line1, = ax.plot([], [], 'b-', label='Training Loss')\n",
    "    line2, = ax.plot([], [], 'r-', label='Validation Loss')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Training and Validation Loss')\n",
    "    ax.legend()\n",
    "\n",
    "    # Initialize the plot\n",
    "    plt.ion()\n",
    "    plt.show()\n",
    "\n",
    "    pbar = tqdm(range(steps))\n",
    "    for i in pbar:\n",
    "        \n",
    "\n",
    "        predictions = mlp.forward(data_x)\n",
    "        np.set_printoptions(precision=3)\n",
    "        np.set_printoptions(suppress=True)\n",
    "        loss = ce_loss(labels, predictions)\n",
    "\n",
    "        # display loss in tqdm bar\n",
    "        tqdm.set_description(pbar, f'Loss: {loss}')\n",
    "        # print(f'Loss: {loss}')\n",
    "        losses.append(loss)\n",
    "        mlp.backward(labels)\n",
    "        optimizer.step()\n",
    "        \n",
    "        v_predictions = mlp.forward(validation_x)\n",
    "        v_loss = ce_loss(validation_y, v_predictions)\n",
    "        v_losses.append(v_loss)\n",
    "\n",
    "        if i % 5 == 0:\n",
    "              line1.set_xdata(np.arange(len(losses)))\n",
    "              line1.set_ydata(losses)\n",
    "              line2.set_xdata(np.arange(len(v_losses)))\n",
    "              line2.set_ydata(v_losses)\n",
    "              ax.relim()\n",
    "              ax.autoscale_view()\n",
    "              \n",
    "              # Update the plot in Jupyter Notebook\n",
    "              fig.canvas.draw()\n",
    "              fig.canvas.flush_events()\n",
    "\n",
    "              display(fig)\n",
    "              clear_output(wait=True)\n",
    "              plt.pause(0.001)\n",
    "    \n",
    "    # Close the plot\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "    \n",
    "    loss_dict = {\n",
    "        'train': losses,\n",
    "        'validation': v_losses\n",
    "    }\n",
    "    plot_losses(loss_dict)\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 1: MLP Layer Depth\n",
    "- models:\n",
    "  - no hidden layers\n",
    "  - single hidden layer, ReLU\n",
    "  - two hidden layers, Relu\n",
    "- for each experiment with\n",
    "  - 32, 64, 128, 256\n",
    "- Output layer is softmax\n",
    "- compare test accuracy\n",
    "- comment on non-linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(depth, width, input_size, output_size):\n",
    "    layers = []\n",
    "    for i in range(depth - 1):\n",
    "        # if this is the last layer, we need to set the output size instead\n",
    "        # if this is the first layer we need to set the input size to the input size\n",
    "        layer_input_size = width if i > 0 else input_size\n",
    "        layer_output_size = width if i < depth - 2 else output_size\n",
    "        layers.append(LinearLayer(layer_input_size, layer_output_size))\n",
    "        if i < depth - 2:\n",
    "            layers.append(ReLULayer())\n",
    "\n",
    "    \n",
    "\n",
    "    layers.append(SoftmaxOutputLayer())\n",
    "    return MLP(*layers)\n",
    "\n",
    "\n",
    "\n",
    "def experiment1():\n",
    "    \n",
    "    # model params:\n",
    "    model_param_tests = [(4,32)]\n",
    "    # for depth in range(3, 5):\n",
    "    #     for width in [32, 64, 128, 256]:\n",
    "    #         model_param_tests.append((depth, width))\n",
    "\n",
    "\n",
    "    MLPs = { params : {'model':get_model(*params, 784, 24)} for params in model_param_tests}\n",
    "    # MLPs = [get_model(*params, 784, 24) for params in model_param_tests]\n",
    "\n",
    "    for mlp in MLPs.values():\n",
    "        # make the training data linear\n",
    "        # x shape: (27455, 28, 28, 1)\n",
    "        # we need to flatten the data\n",
    "        # x_flat shape: (27455, 784)\n",
    "        X_t_flat = X_t.reshape(X_t.shape[0], -1)\n",
    "        mlp['losses'] = train(mlp['model'], GradientDescentOptimizer(mlp['model'], 0.3), X_t_flat, Y_t, 500)\n",
    "        # calculate the accuracy test and train accuracy\n",
    "        # evaluation accuracy\n",
    "        print('Evaluating model...')\n",
    "        eval_acc = np.mean(np.argmax(mlp['model'].forward(X_e.reshape(X_e.shape[0], -1)), axis=1) == np.argmax(Y_e, axis=1))\n",
    "        print(f'Eval Accuracy: {eval_acc}')\n",
    "        mlp['eval_acc'] = eval_acc\n",
    "        train_acc = np.mean(np.argmax(mlp['model'].forward(X_t_flat), axis=1) == np.argmax(Y_t, axis=1))\n",
    "        print(f'Train Accuracy: {train_acc}')\n",
    "        mlp['train_acc'] = train_acc\n",
    "    \n",
    "    return MLPs\n",
    "\n",
    "\n",
    "MLPs = experiment1()\n",
    "\n",
    "for params, mlp in MLPs.items():\n",
    "    print(f'Model: {params}')\n",
    "    print(f'Eval Accuracy: {mlp[\"eval_acc\"]}')\n",
    "    print(f'Train Accuracy: {mlp[\"train_acc\"]}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 2: MLP activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 3: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 4: Convultional Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 5: Optimizing MLP Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp. 6: Report Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results And Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
